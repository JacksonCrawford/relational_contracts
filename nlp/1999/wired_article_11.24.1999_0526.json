{"URL": "https://www.wired.com/1999/11/super-savings-on-a-supercomputer", "heading": "super savings on a supercomputer", "subheading": "a home-grown supercomputer shows it can compete with the biggest and baddest supercomputers in the world -- and at a fraction of the cost. by kristen philipkoski.", "author": "kristen philipkoski", "category": "science", "type": "article", "timestamp": "11.24.1999 03:00 AM", "text": "among the world's fastest-500 supercomputers, cplant cluster -- the one at  sandia national labs in albuquerque, new mexico -- ranks number 44.the 44th spot might not sound like a big deal, but sandia computer researchers say otherwise -- mainly because cplant is one of the few on the list that are \"homemade.\"at us$7.5 million, cplant is less than 10 percent the cost of more specialized supercomputers, such as ibm and lawrence livermore national lab's blue pacific, which cost us$94 million.\"what's different about cplant is that it's sewn from commodity computers instead of being grown from the ground up as a monolithic supercomputer,\" said rolf reisen, senior member of the technical staff and the investigator who developed cplant at sandia.cplant is an assembly of 600 computers that are each about the same as any high-end system you'd find in a computer store, according to reisen.\"what's different from all these others is we select these individual pieces and ask someone else [in this case compaq] to put it together. what that opens up is for us to buy the pieces in one place and have somebody else put it together, which helps keep costs down,\" reisen said.alone, each of cplant's computers performs about 600 million operations per second. together, the 600 computers employ 580 nodes or \"computer brains\" to perform 232.6 billion operations per second.the sandia scientists wrote customized communication software to get the off-the-shelf computers talking at maximum speed. then they developed their own utilities and used the linux operating system for networking.the scalable cluster is easily made even more powerful by adding more pcs to cplant.the top 500 list is maintained by a researcher at oak ridge national labs in tennessee, and is updated every six months when a benchmark test is run on supercomputers around the world to determine which perform the best.the rankings are announced at a supercomputer conference every november in portland, oregon, and at another conference in the spring in germany.the top 500 supercomputers web site lists supercomputers from intel, ibm, cray-sgi, fujitsu, and hitachi.reisen also helped develop teraflops, which, as the fastest supercomputer in the world, gets no. 1 ranking on the top 500 list. it solves problems at more than a trillion operations per second.\"cplant is now up there with all of these other supercomputers,\" reisen said.this year, cluster computers made of off-the-shelf components like cplant were the buzz at the portland meeting.\"commodity supercomputers are one of the hot research areas in supercomputing. i believe the driving force is bringing down the cost of computing for a portion of the market space,\" said peter ungaro, manager of ibm's worldwide research center.ibm is developing its own off-the-shelf scalable supercomputer in conjunction with argonne national labs in illinois.the project, called chiba city, will be the largest scalable supercomputing cluster dedicated to open source software development, according to argonne. the system will be available for use by researchers at universities, laboratories, and private companies.the supercomputers can be used to run programs such as simulations of airplane crashes, safety assessments of weapons, or shipments of nuclear waste.\"they simulate what happens if the nuclear waste container gets in an accident or breaks,\" sandia's reisen said.\"[sandia's goal] is to figure out how to build high-capacity machines and their approach is to maximize the [operations] per dollar,\" said mark seager, assistant department head for terascale systems livermore.livermore lab's own version of cluster computers are geared toward performance rather than bringing down cost. livermore has spot no. 2 on the top 500 supercomputers list with blue pacific followed by los alamos national labs' blue mountain at no. 3.because the off-the-shelf cluster supercomputers are cheaper to use, they could open up the use of supercomputers to more researchers and smaller companies.\"we're starting to offer a chance for smaller companies or labs to have a chance to run some of these things since there will be more capacity,\" reisen said. \"you can run codes you would have avoided before, because time on a supercomputer is usually so precious.\""}