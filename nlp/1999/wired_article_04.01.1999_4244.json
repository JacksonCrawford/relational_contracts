{"URL": "https://www.wired.com/1999/04/y2k", "heading": "the myth of order", "subheading": "the real lesson of y2k is that software operates just like any natural system: out of control. y2k has uncovered a hidden side of computing. it\u2019s always been there, of course, and always will be. it\u2019s simply been obscured by the pleasures we get from our electronic tools and toys, and then lost in the [\u2026]", "author": "ellen ullman", "category": "not found", "type": "article", "timestamp": "04.01.1999 12:00 PM", "text": "the real lesson of y2k is that software operates just like any natural system: out of control. y2k has uncovered a hidden side of computing. it's always been there, of course, and always will be. it's simply been obscured by the pleasures we get from our electronic tools and toys, and then lost in the zingy glow of techno-boosterism. y2k is showing everyone what technical people have been dealing with for years: the complex, muddled, bug-bitten systems we all depend on, and their nasty tendency toward the occasional disaster.it's almost a betrayal. after being told for years that technology is the path to a highly evolved future, it's come as something of a shock to discover that a computer system is not a shining city on a hill - perfect and ever new - but something more akin to an old farmhouse built bit by bit over decades by nonunion carpenters.the reaction has been anger, outrage even - how could all you programmers be so stupid? y2k has challenged a belief in digital technology that has been almost religious. but it's not surprising. the public has had little understanding of the context in which y2k exists. glitches, patches, crashes - these are as inherent to the process of creating an intelligent electronic system as is the beauty of an elegant algorithm, the satisfaction of a finely tuned program, the gee-whiz pleasure of messages sent around the world at light speed. until you understand that computers contain both of these aspects - elegance and error - you can't really understand y2k.\"bugs are an unintended source of inspiration. many times i've seen a bug in a game and thought, 'that's cool - i wouldn't have thought of that in a million years.'\" - will wright, creator of simcity and chief game designer at maxis\"i've fixed about 1,000 bugs in my life. how many have i created? undoubtedly more.\" - patrick naughton, executive vice president of products, infoseektechnically speaking, the \"millennium bug\" is not a bug at all, but what is called a design flaw. programmers are very sensitive to the difference, since a bug means the code is at fault (the program isn't doing what it was designed to do), and a design flaw means it's the designer's fault (the code is doing exactly what was specified in the design, but the design was wrong or inadequate). in the case of the millennium bug, of course, the code was designed to use two-digit years, and that's precisely what it's doing. the problem comes if computers misread the two-digit numbers - 00, 01, et cetera. should these be seen as 1900 and 1901, or as 2000 and 2001? two-digit dates were used originally to save space, since computer memory and disk storage were prohibitively expensive. the designers who chose to specify these two-digit \"bugs\" were not stupid, and perhaps they were not even wrong. by some estimates, the savings accrued by using two-digit years will have outweighed the entire cost of fixing the code for the year 2000.but y2k did not even begin its existence as a design flaw. up until the mid-1980s - almost 30 years after two-digit years were first put into use - what we now call y2k would have been called an \"engineering trade-off,\" and a good one. a trade-off: to get something you need, you give up something else you need less urgently; to get more space on disk and in memory, you give up the precision of the century indicators. perfectly reasonable. the correct decision. the surest sign of its correctness is what happened next: two-digit years went on to have a long, successful life as a \"standard.\" computer systems could not work without standards - an agreement among programs and systems about how they will exchange information. dates flowed from program to program, system to system, from tape to memory to paper, and back to disk - it all worked just fine for decades.though not for centuries, of course. the near immortality of computer software has come as a shock to programmers. ask anyone who was there: we never expected this stuff to still be around.bug, design flaw, side effect, engineering trade-off - programmers have many names for system defects, the way eskimos have many words for snow. and for the same reason: they're very familiar with the thing and can detect its fine gradations. to be a programmer is to develop a carefully managed relationship with error. there's no getting around it. you either make your accommodations with failure, or the work will become intolerable. every program has a bug; every complex system has its blind spots. occasionally, given just the right set of circumstances, something will fail spectacularly. there is a silicon valley company, formerly called failure analysis (now exponent), whose business consists of studying system disasters. the company's sign used to face the freeway like a warning to every technical person heading north out of silicon valley: failure analysis.no one simply accepts the inevitability of errors - no honest programmer wants to write a bug that will bring down a system. both engineers and technical managers have continually looked for ways to normalize the process, to make it more reliable, predictable - schedulable, at the very least. they have talked perennially about certification programs, whereby programmers would have to prove minimal proficiency in standard skills. they have welcomed the advent of reusable software components, or \"objects,\" because components are supposed to make programming more accessible, a process more like assembling hardware than proving a mathematical theorem. they've tried elaborate development methodologies. but the work of programming has remained maddeningly undefinable, some mix of mathematics, sculpting, scrupulous accounting, and wily, ingenious plumbing.in the popular imagination, the programmer is a kind of traveler into the unknown, venturing near the margin of mind and meatspace. maybe. for moments. on some extraordinary projects, sometimes - a new operating system, a newly conceived class of software. for most of us, though, programming is not a dramatic confrontation between human and machine; it's a confused conversation with programmers we will never meet, a frustrating wrangle with some other programmer's code.\"january 1 is a saturday. so if the world comes to an end for a couple of days, it'll be ok. we've all had weekends like that.\" - reed hundt, former fcc chair\"one guy in our office keeps a wooden head at the top of his cube - the god of debugging. he makes offerings to it daily.\" - maurice doucet, director of engineering at metacreationsmost modern programming is done through what are called application programming interfaces, or apis. your job is to write some code that will talk to another piece of code in a narrowly defined way using the specific methods offered by the interface, and only those methods. the interface is rarely documented well. the code on the other side of the interface is usually sealed in a proprietary black box. and below that black box is another, and below that another - a receding tower of black boxes, each with its own errors. you can't envision the whole tower, you can't open the boxes, and what information you've been given about any individual box could be wrong. the experience is a little like looking at a madman's electronic bomb and trying to figure out which wire to cut. you try to do it carefully but sometimes things blow up.at its core, programming remains irrational - a time-consuming, painstaking, error-stalked process, out of which comes a functional but flawed piece of work. and it most likely will remain so as long as we are using computers whose basic design descends from eniac, a machine constructed to calculate the trajectory of artillery shells. a programmer is presented with a task that a program must accomplish. but it is a task as a human sees it: full of unexpressed knowledge, implicit associations, allusions to allusions. its coherence comes from knowledge structures deep in the body, from experience, memory. somehow all this must be expressed in the constricted language of the api, and all of the accumulated code must resolve into a set of instructions that can be performed by a machine that is, in essence, a giant calculator. it shouldn't be surprising if mistakes are made.there is irrationality at the core of programming, and there is irrationality surrounding it from without. factors external to the programmer - the whole enterprise of computing, its history and business practices - create an atmosphere in which flaws and oversights are that much more likely to occur.the most irrational of all external factors, the one that makes the experience of programming feel most insane, is known as \"aggressive scheduling.\" whether software companies will acknowledge it or not, release schedules are normally driven by market demand, not the actual time it would take to build a reasonably robust system. the parts of the development process most often foreshortened are two crucial ones: design documentation and testing. i recently went to a party where a senior consultant - a woman who has been in the business for some 30 years, someone who founded and sold a significant software company - was explaining why she would no longer work with a certain client. she had presented a software development schedule to the client, who received it, read it, then turned it back to her, asking if she'd remake the schedule so that it took exactly half the time. there were many veteran programmers in the room; they nodded along in weary recognition.even if programmers were given rational development schedules, the systems they work on are increasingly complex, patched together - and incoherent. systems have become something like russian nesting dolls, with newer software wrapped around older software, which is wrapped around software that is older yet. we've come to see that code doesn't evolve; it accumulates.a young web company founder i know - very young; scott hassan of egroups.com - suggests that all programs should be replaced every two years. he's probably right. it would be a great relief to toss all our old code into that trash container where we dumped the computer we bought a couple of years ago. maybe on the web we can constantly replenish our code: the developer never lets go of the software; it sits there on the server available for constant change, and the users have no choice but to take it as it comes.but software does not follow moore's law, doubling its power every 18 months. it's still the product of a handworked craft, with too much meticulous effort already put into it. even egroups.com, founded only nine months ago, finds itself stuck with code programmers have no time to redo. said carl page, another of its founders, \"we're living with code we wish we'd done better the first time.\"\"debugging had to be discovered. i can remember the exact instant when i realized that a large part of my life from then on was going to be spent finding mistakes in my own programs.\" - maurice wilkes, creator of the edsac and consultant to olivetti research lab\"trust the computer industry to shorten 'year 2000' to 'y2k.' it was this kind of thinking that caused the problem in the first place.\" - anonymous net wisdomthe problem of old code is many times worse in a large corporation or a government office, where whole subsystems may have been built 20 or 30 years ago. most of the original programmers are long gone, taking their knowledge with them - along with the programmers who followed them, and ones after that. the code, a sort of palimpsest by now, becomes difficult to understand. even if the company had the time to replace it, it's no longer sure of everything the code does. so it is kept running behind wrappers of newer code - so-called middleware, or quickly developed user interfaces like the web - which keeps the old code running, but as a fragile, precious object. the program runs, but is not understood; it can be used, but not modified. eventually, a complex computer system becomes a journey backward through time. look into the center of the most slick-looking web banking site, built a few months ago, and you're bound to see a creaky database running on an aged mainframe.adding yet more complexity are the electronic connections that have been built between systems: customers, suppliers, financial clearinghouses, whole supply chains interlinking their systems. one patched-together wrapped-up system exchanges data with another patched-together wrapped-up system - layer upon layer of software involved in a single transaction, until the possibility of failure increases exponentially.it's from deep in there - somewhere near the middle-most russian doll in the innermost layer of software - that the millennium bug originates. one system sends it on to the next, along with the many bugs and problems we already know about, and the untold numbers that remain to be discovered. one day - maybe when we switch to the new version of the internet protocol, or when some router somewhere is replaced - one day the undiscovered bugs will come to light and we'll have to worry about each of them in turn. the millennium bug is not unique; it's just the flaw we see now, the most convincing evidence yet of the human fallibility that lives inside every system.it's hard to overstate just how common bugs are. every week, the computer trade paper infoworld prints a little box called \"the bug report,\" showing problems in commonly used software, some of them very serious. and the box itself is just a sampling from www.bugnet.com, where one day's search for bugs relating to \"security\" yielded a list of 68 links, many to other lists and to lists of links, reflecting what may be thousands of bugs related to this keyword alone. and that's just the ones that are known about and have been reported.if you think about all the things that can go wrong, it'll drive you crazy. so technical people, who can't help knowing about the fragility of systems, have had to find some way to live with what they know. what they've done is develop a normal sense of failure, an everyday relationship with potential disaster.one approach is to ignore all thoughts about the consequences - to stay focused on the code on your desk. this is not that difficult to do, since programmers get high rewards for spending large amounts of time in front of a computer workstation, where they're expected to maintain a very deep and narrow sort of concentration. a few months ago, i talked to a systems programmer who'd barely looked over the top of his cubicle for 30 years. he'd spent half that time working in the federal reserve system, backbone of the world banking order everyone fears will collapse come the millennium. but until he joined the fed's y2k project, he had never much considered the real-world effects of his work. \"i read an article about how the federal reserve would crash everything if it went bad,\" said the man i'll call jim fuller, who agreed to talk only on condition of anonymity. \"it was the first time in my life i understood everything the federal reserve did.\" he'd taken a rare look up and down the supply chain; the job of fixing y2k in the context of an enormous, linked economic machine was now a task that stretched out in all directions far beyond his control. it scared him. \"i discovered we were kind of important,\" he said uneasily.if you can't stay focused on your code, another approach is to develop an odd sort of fatalism, a dark, defensive humor in the face of all the things you know can go wrong. making fun of bugs is almost a sign of sophistication. it shows you know your way around a real system, that you won't shy back when things really start to fall apart. a friend of mine once worked as a software engineer at a baby bell. he liked to tell people how everyone in the company was amazed to pick up a handset and actually get a dial tone. it was almost a brag: ha ha, my system's so screwed up you wouldn't believe it.now here comes a problem that's no joke. technical people can't help hearing about the extreme consequences that will come down on the world if they don't find all the places y2k is hiding. and they simultaneously know that it is impossible to find all the problems in any system, let alone in ones being used long beyond their useful life spans. programmers feel under siege, caught between the long-standing knowledge of error and fragility they've learned to live with, and the sudden, unrealistic pressure to fix everything.\"to paraphrase mark twain, the difference between the right program and almost the right program is like the difference between lightning and a lightning bug. the difference is just a bug.\" - danny hillis, in the pattern on the stone (1998)\"i am one of the culprits who created the problem. i used to write those programs back in the '60s and '70s, and was so proud of the fact that i was able to squeeze a few elements of space by not having to put '19' before the year.\" - alan greenspan, federal reserve chair\"y2k is a sort of perverse payback from the universe for all the hasty and incomplete development efforts over the last 10 years,\" said the y2k testing lead for a midsize brokerage. also speaking on condition of anonymity, lawrence bell (a pseudonym) said it like an i-told-you-so, a chance for him to get back at every programmer and programming manager who ever sent him junky software.bell is a tall, impeccably groomed young man whose entire workday consists of looking for bugs. he's in qa, quality assurance, the place where glitches are brought to light, kept on lists, managed, prioritized, and juggled - a complete department devoted to bugs. he has the tester's crisp manner, the precision of the quality seeker, in whom a certain amount of obsessive fussiness is a very good thing. since bell doesn't write code, and can't just concentrate on the program on his desk, he has no alternative but to affect a jaunty, fake cheer in the face of everything that can go wrong. \"we have systems that have been developed in, shall we say, an 'uncontrolled' manner,\" he said.the systems he's responsible for testing are classic journeys through time: new systems on windows nt with graphical user interfaces, unix relational databases on the sturdy client-server systems of the late '80s, command-line interfaces that were in vogue in the late '70s and early '80s, all the way back to an ibm midrange computer running programs \"that nobody thinks about,\" said bell, but \"have to run or we're in trouble.\"bell's team is doing what they call \"clean management\": testing everything for y2k problems, whether or not they suspect it has a date-related problem. in the course of it, as they go backward in time, they're coming across systems that have never been formally tested. \"there was a day when things did not go through qa,\" said bell, as if he were talking about another century. all this time, the untested systems have been out there, problems waiting to happen. \"we find all sorts of functional bugs,\" he said affably. \"not y2k. just big old bugs.\"bell had all the complaints testers always have. missing source code. no documentation. third-party software vendors who won't give them information. not enough people who know how the systems were put together. users who won't take the time to explain how they work with the system. and what he calls the \"ominous task\" of fixing one of the oldest, least documented systems - the crucial trade-clearing system running on the ibm machines. \"if one of the midrange computers goes down for a day, we're out of business without our backups,\" he said.still, quality assurance is the one place where the muddled side of computing is obvious, predominant, inescapable. bell, as a good qa guy, is mostly inured to it all. \"come the year 2000, a couple of systems will fail,\" he said nonchalantly. \"but that's what happens with any implementation. it's the same thing we've been doing for years.\"for bell, it's no big deal that supposedly y2k-compliant programs will be put into users' hands without thorough testing. he's comfortable with the idea that things can go very, very wrong and still not bring about the end of the world. said bell with a shrug, \"it's just a big user test.\"\"we used to have 'bugs for bucks' prizes, because toward the end of debugging, the bugs get hard to find. we'd add $10 to the prize for each bug found. but then people would hold off reporting one until the price went up. it was an underground economy in bug reporting.\" - heidi roizen, former vp of developer relations at applethe millennium bug is not unique - human fallibility lives inside every system.the only thing about y2k that was really bothering lawrence bell was the programmers. there is a classic animosity between programmer and tester - after all, the tester's role in life is to find everything the programmer did wrong. but y2k and its real-world time pressures seem to have escalated the conflict. bell thought that qa would manage - \"it won't be pretty but we'll do it\" - but no thanks to the programmers who developed the applications. \"the application folks are never there,\" said bell, deeply annoyed. \"we're not getting analysis from the developers - it's really absurd.\"the source of the hostility is documentation: programmers are supposed to make a record of the code they've written. documentation is how qa people know what the system is supposed to do, and therefore how to test it. but programmers hate to write documentation, and so they simply avoid doing it. \"the turnover is high,\" said bell, \"or the programmers who have been here a long time get promoted. they don't want to go back to this project they wrote 10 years ago - and get punished for not documenting it.\"programmers have fun and leave us to clean up their messes, is bell's attitude. they want to go off to new programs, new challenges, and the really annoying thing is, they can. \"they say, 'i want to do something new,'\" said bell, truly angry now, \"and they get away with it.\"\"no more programmers working without adult supervision!\"this was declaimed by ed yardeni, chief economist for deutsche bank securities, before a crowded hotel ballroom. on the opening day of the year 2000 symposium, august 10, 1998 (with cameras from 60 minutes rolling), yardeni explained how the millennium bug would bring about a world recession on the order of the 1973-74 downturn, and this would occur because the world's systems \"were put together over 30 to 40 years without any adult supervision whatsoever.\" blame the programmers. the mood at the conference was like that of a spurned lover: all those coddled boys in t-shirts and cool eyewear, formerly fetishized for their adolescent ways, have betrayed us.it has become popular wisdom to say that y2k is the result of \"shortsightedness.\" it's a theme that has been taken up as a near moral issue, as if the people who created the faulty systems were somehow derelict as human beings.in fact, some of the most successful and long-lived technologies suffer from extreme shortsightedness. the design of the original ibm pc, for example, assumed there would never be more than one user, who would never be running more than one program at a time, which would never see more than 256k of memory. the original internet protocol, ip, limited the number of server addresses it could handle to what seemed a very large number at the time, never imagining the explosive growth of the web.i once worked on a cobol program that had been running for more than 15 years. it was written before the great inflation of the late 1970s. by the time i saw it, in 1981, the million-dollar figure in all dollar amounts was too large for the program's internal storage format, and so multiple millions of dollars simply disappeared without a trace.we are surrounded by shortsighted systems. right at this moment, some other program is surely about to burst the bounds of its format for money or number of shares traded or count of items sold. the dow jones industrial average will one day break 10,000, the price of gas will top $9.99, the systems we're renovating now may live long enough to need renovation again. some system designer, reacting to the scarce computer resource of our day - not memory but bandwidth - is specifying a piece of code that we will one day look back on as folly.at the year 2000 symposium where yardeni spoke, there was a technical workshop about creating a \"time machine\" - a virtual time environment for testing \"fixed\" y2k programs. one of the presenters, carl gehr of the edge information group, patiently explained that, when designing the test environment, \"you have to specify an upper limit\" for the year. while everyone scribbled notes, an awful thought occurred to me. \"but what upper limit?\" i said out loud. \"should we be worrying about the year 9000? 10,001?\"gehr stopped talking, heads came up from their notes, and the room went quiet. it was as if this were the first time, in all the rush to fix their systems, the attendees had been able to stop, reflect, think about a faraway future. finally, from the back of the room came a voice: \"good question.\"things can go very, very wrong and still not be the end of the world. says bell: \"it's just a big user test.\"gehr glanced over at his colleague, marilyn frankel, who was waiting to talk about temporary \"fixes\" for y2k-affected code. \"marilyn will address that later, i'm sure,\" he said."}