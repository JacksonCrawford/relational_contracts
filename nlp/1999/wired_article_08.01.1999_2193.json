{"URL": "https://www.wired.com/1999/08/akamai", "heading": "the new cool", "subheading": "proof of concept akamai overcomes the internet\u2019s hot spot problem. paul sagan says that danny could leave the company to finish his phd and publish his thesis, but then they\u2019d have to kill him. everyone else at akamai is encouraged to complete their academic work, a slew of them at mit, but danny \u2013 him [\u2026]", "author": "paul spinrad", "category": "not found", "type": "article", "timestamp": "08.01.1999 12:00 PM", "text": "proof of conceptakamai overcomes the internet's hot spot problem.paul sagan says that danny could leave the company to finish his phd and publish his thesis, but then they'd have to kill him. everyone else at akamai is encouraged to complete their academic work, a slew of them at mit, but danny - him they'd have to off. he knows too much.danny lewin is an algorithms guy, and at akamai technologies, algorithms rule. after years of research, he and his adviser, professor tom leighton, have designed a few that solve one of the direst problems holding back growth of the internet. this spring, tom and danny's seven-month-old company launched a service built on these secret formulas.the akamai solution recalls great historical shifts - discoveries of better, faster ways - like the invention of arabic numerals, or the development of seafaring. take the latter: for most of prehistory, people traveled exclusively over land. then, around 5,000 years ago, they discovered that floating cargo on water was easier than lugging it across terrain - no mountains to climb, no roads to negotiate. seafaring transformed most of the world's surface from unusable space into a vast ubiquitous shortcut, a portal to faraway lands and great riches. natural harbors blossomed into sophisticated cities. and although societies continued exchanging goods with their neighbors over land, the first world powers, the first empires, commanded the seas.in some ways, sending information around the traditional internet resembles human transport, pre-phoenicia. the net was originally designed like a series of roads connecting distinct sources of content. different servers, physical hardware, specialized in their own individual data domains. as first conceived, an address like nasa.gov would always correspond to dedicated servers located at a nasa facility. when you visited www.ksc.nasa.gov to see a shuttle launch, you connected to nasa's servers at kennedy space center, just as you traveled to tivoli for travertine marble instead of picking it up at your local port. when you ran a site, your servers and only your servers delivered its content.this routing system worked fine for years, but as users move to fatter pipes, like dsl and broadband cable, and as event-driven supersites emerge, the protocols tying information to location cause a bottleneck. back when the starr report was posted, congress' servers couldn't keep up with hungry surfers. when victoria's secret ran its super bowl ad last february, similar lusts went unsated. the heaven's gate site in 1997 quickly followed its cult members into oblivion. and when the phantom menace trailers hit the web this spring, a couple of sites distributing them went down.this is the \"hot spot\" problem: when too many people visit a site, the excessive load heats it up like an overloaded circuit and causes a meltdown. just as something on the net gets interesting, access to it fails.for more time-critical applications, the stakes are higher. when the stock market lurches and online traders go berserk, brokerage sites can hardly afford to buckle. in retail, slow responses will send impatient customers clicking over to the competition. users may have pentium iiis and isdn lines, but when a site can't keep up with demand, they feel like they're on a slow dialup. and users on relatively remote parts of the network - even tech hubs like singapore - often suffer slow responses, not just during peak traffic.isps address this problem by adding connections, expanding capacity, and running server farms to host client sites on many machines, but this still leaves content clustered in one place on the network. publishers can mirror sites at multiple hosting companies, helping to spread out traffic, but this means duplicating everything everywhere, even the files no one wants. a third remedy, caching, temporarily stores copies of popular files on servers closer to the user, but out of the original site's control. naturally, site publishers don't like this - it delivers stale content, preserves errors, and skews usage stats. in other words, massive landlock.so in 1998, with their new algorithms in hand, tom leighton and danny lewin found themselves facing a sort of manifest destiny. the web's largest sites were straining to meet demand - and frequently failing. most needed better traffic handling, a way to cool down hot spots and speed content delivery overall. and tom and danny had conceived a solution, a grand-scale alternative to the net's routing system.in september, a california company called sandpiper networks introduced a service perilously similar to what they'd envisioned, but tom and danny's load-balancing solution was one step more radical, and the problem was plenty big for two contenders. paul sagan, a content guy from time warner's pathfinder, signed up to lead them, and the cambridge, massachusetts-based startup began building its own globe-spanning network of servers that would handle web content in a brand-new way.it worked. with akamai's freeflow service, all content pours through the entire network, instantly responding to demand, ebbing and flowing as needed, changing routes and locations in response to current conditions. its ocean of servers connects to the terra firma of the rest of the net at scores of ports, all of which move data more quickly as conditions continually change.no fixed addressin january, akamai began running beta versions of freeflow, serving content for espn.com, paramount pictures, apple, and other high-volume clients. (akamai withholds the names of the others, but you can tell if a site is using the service by viewing the page source and looking for akamaitech.net in the urls. a cursory test reveals \"akamaized\" content at yahoo! and geocities.)espn.com and paramount have been good beta testers - espn.com because it requires frequent updates and is sensitive to region as well as time, and paramount because it delivers a lot of pipe-hogging video. on march 11, while espn was covering the first day of ncaa hoops' march madness, paramount's entertainment tonight online posted the second phantom menace trailer. freeflow handled up to 3,000 hits per second for the two sites - 250 million in total, many of them 25-mbyte downloads of the trailer. but the system never exceeded even 1 percent of its capacity. in fact, as the download frenzy overwhelmed other sites, akamai picked up the slack. before long, akamai became the exclusive distributor of all phantom menace quicktimes, serving both of the official sites, starwars.com and apple.com.so how does it work? companies sign up for akamai's freeflow, agreeing to pay according to the amount of their traffic. then they run a simple utility to modify tags, and the akamai network takes over. throughout the site, the system rewrites the urls of files, changing the links into variables to break the connection between domain and location. on www.apple.com, for example, the link www.apple.com/home/media/menace_640qt4.mov, specifying the 640 x 288 phantom menace quicktime trailer, might be rewritten as a941.akamai.com/7/941/51/256097340036aa/www.apple.com/home/media/menace_640qt4.mov. under standard protocols, a941.akamaitech.net would refer to a particular machine. but with akamai's system, the address can resolve to any one of hundreds of servers, depending on current conditions and where you are on the net. and it can resolve a different way for someone else - or even for you, a few seconds later. (the /7/941/51/256097340036aa in the url is a fingerprint string used for authentication.) this new method is more complicated, but like modern navigation, it opens new vistas of capacity and commerce.sandpiper remains akamai's only direct competitor. in april it signed a deal with aol and inktomi to begin serving their sites and incorporating their servers into the sandpiper network. but a month out of the starting gate, akamai was running immediately neck and neck with its rival, both promising more than 1,000 servers by the end of the year.academic hot spotpartly because it arrived second, akamai has had to differentiate its product. the company has done this not only by focusing on fine points of the technology, but also by positioning itself as the intelligent solution. freeflow is the masterwork of leading scientists from mit.what's more, the scientists of akamai are algorithms people. whereas network hackers tend to be masters of improvisation, spotting local problems and using intuition and quick experimentation to fire off fixes, algorithms people tend to be slower and more rigorous, examining and proving everything along the way. they start with the most pared-down problems - sorting numbers, stacking rectangles, connecting dots - and build up to more complicated situations. they study how efficiently computer programs run under all conditions - the best, average, and worst-possible cases - as the mass of processors, connections, and information become infinitely large. it may take them a while to find a solution they like, but when they do, they know it will work, both on paper and in any reality.this is the hot spot phenomenon: just as something on the net gets interesting, access to it fails.so network growth doesn't scare algorithms people; they always push things to infinity anyway.you can trace akamai's genesis to lcs, mit's laboratory for computer science, where tim berners-lee's world wide web consortium and tom leighton's algorithms group both have their offices. in 1995 tim asked tom if he thought distributed algorithms could reliably solve the hot spot problem, and tom's algorithms posse was intrigued. the problem raised interesting theoretical issues, the group's forte, while at the same time the graphs of nodes and edges they drew on whiteboards represented actual machines and net connections. real-world relevance! several semesters and ideas later - some published, some still proprietary - tom and danny had blueprints for server software that would cool down traffic everywhere. but better, they had formally proven that most of their algorithms were optimal. in other words, different solutions might give equally good results, but none could ever possibly do better.for their nonoptimal algorithms, tom and danny demonstrated that the problems were, in computer-science shorthand, \"hard\" problems. this means there are no major shortcuts to finding the best solution; the problem is inherently difficult, and you just have to do the best you can with finite time and computational resources.lastly, tom and danny knew with total certainty that, given their descriptions of the hot spot problem and the workings of the net, the larger the network grew, the better their solution would perform. they not only had a solution, they had a solution that was literally - demonstrably - unbeatable.la sauce est toutthe name akamai means \"clever\" in hawaiian, or more colloquially, \"cool.\" at danny's suggestion, the team found it by trying out words in an online english-hawaiian dictionary. in a sense, freeflow is an attempt to put smartness into the web page, the url, and the network itself.although at first akamai's founders imagined selling their traffic-calming solution as server software - isps would buy it and install it themselves to boost performance - they soon realized they should be a service for publishers, content providers, and ecommerce companies of all kinds, not a software company. with akamai's service, publishers could forget about servers and isps, and concentrate on content. akamai would run its software on its own broadly deployed server network and sell guaranteed fast delivery, subscription style. the idea was to work with as many isps as possible to create a new layer of infrastructure on top of the net, a fluid system that would run everywhere and reach out to remote populations.small isps operate at a single location; the big ones have their own subnetworks encompassing multiple pops in different locations. both are basically facilities where a bunch of servers share one or more network connections. akamai calls these facilities \"regions,\" and to get close to all users, the company tries to locate its servers in as many regions as it can. singapore's singnet has akamai servers in its single region, while teleglobe has them all over. partnering isps small and large benefit from improved capacity, while their users get faster delivery of freeflow sites. (the company targets content providers and ecommerce sites, but hopes to cultivate an intel-style status among consumers as well, who would look for isps with \"akamai inside.\")so akamai servers are in diaspora, but they remain clannish. they keep in constant contact with each other all over the map, speaking their own special dialect of linux. each region has one mapping server and one or more content servers. all content servers, no matter where they are, are eligible to serve any content. the mapping servers monitor the local state of the network: how fast are the current connections to neighboring regions? which connections seem to have gone down completely? they figure out which servers should carry which files, and then how to evenly distribute the hits for a requested file among the servers that carry it.web pages themselves break down into units: the html page plus each embedded file it contains - images, animations, sounds, video. akamai's system wisely leaves the html alone and scatters only the embedded elements, the rationale being that these larger files cause the traffic jams, while plain html is fast and cheap. in a big cafeteria, freeflow might pick up your meat loaf, green beans, and rice pudding at three different counters, calculating which steam tables look hotter and where the lines are long. but your tray, the html, always comes from the same place. keeping the html on the home server also keeps the user database and customization scripts in one place, where publishers want them.when a user requests a file, the mapping servers decide on a content server in two stages: they choose first the best region and then the content server within that region. in computer-science terms, the first stage represents a classic \"min-cost\" flow problem, where the cost associated with each hop between neighboring regions - or how easy it is for traffic to flow between them - is weighted. as traffic conditions change, the mapping servers update these weights and continually find a low-cost route based on a user's place on the network. as akamai and sandpiper both know, this is an expensive, \"hard\" calculation. the mapping servers have to be fast.within a region, for the second stage of routing, the system divides the traffic evenly among all the servers using \"consistent hashing,\" a wonderful double-randomized hashing algorithm that danny invented, earning him mit's 1998 morris joseph lewin award (no relation) for best master's thesis. simple hashing algorithms, which assign objects to locations the way a card dealer deals out hands, break down completely when players drop out or come in; the original formula for who will receive what card, which relies on knowing the number of players, no longer works. but consistent hashing splits up and mixes the assignments so thoroughly, while still using a fairly simple formula, that if locations drop out and throw things off, the correct location will still be close by. as more server problems and network glitches arise, the algorithm has to do more second-guessing, but it achieves the best results possible in an unsteady environment.routing and spreading the hits intelligently is important, but it isn't the whole solution. the real hot spot cooler is balancing the content load - determining which servers should have which files in the first place, before they fulfill requests routed to them. akamai's solution replicates the popular files on multiple servers, spreads total loads evenly, and minimizes copying files around. the ingenious algorithm underlying this process is akamai's secret sauce, and as the french say, the sauce is everything.it was a solution that was literally - demonstrably - unbeatable.cambridge donsunderlying algorithms aside, a private network service business is a different beast entirely from a software business - for one thing, it requires far more money.discovered by battery ventures through mit's student-run \"$50k\" entrepreneurship competition (the akamai team failed to win the trial's whopping $50,000 grant for starting a new business, though the winner turned out virtuous and offered to split the take with the four other finalists), akamai later secured funding from several angel investors. among them were gil friesen (formerly of classic sports) and art bilger (formerly of apollo fund and new world communications). polaris venture partners of boston and seattle also joined, and by the end of 1998 the startup had more than $8 million in first-round funding.the talent snowballed. battery ventures' todd dagres recruited paul sagan as chief operating officer last january. in march david goodtree joined as head of marketing, after years of studying the it industry at forrester research. in april george conrades became akamai's ceo and chair. at bbn george had overseen the acquisition of genuity, authors of the traffic-management software tool hopscotch, and when he learned about akamai, it seemed the perfect big-thinking, out-of-the-box idea: a hopscotch for the entire internet. everyone believed.today akamai employs about 130 people. many are mit students, both graduate and undergrad. but alongside them in the trenches is a surprising number of actual faculty on temporary leave - full professors and associate professors from places like mit, carnegie mellon, and uc berkeley.the sandpiper approachakamai and sandpiper are not the first in their field of distributed traffic management. older systems, many still available and useful, perform sophisticated routing and load balancing over groups of servers installed on a single subnetwork. companies like cisco (distributeddirector), gte internetworking (which acquired bbn and with it genuity's hopscotch), and resonate (central dispatch) have been selling such solutions as installable software or hardware. digex and gte internetworking (web advantage) offer hosting that uses intelligent load balancing and routing within a single isp. these work like akamai's and sandpiper's services, but with a narrower focus.but only akamai and sandpiper are selling the service of whole server networks spanning numerous isps, and only akamai and sandpiper use the trick of rewriting urls as a hook into the alternative system.both companies' services are powerful, but they're not identical. footprint customers specify which part of their content the system should handle by defining rules through a user interface, rather than by adding tag lines to page source. sites project expected traffic levels ahead of time and pay for levels of service, rather than paying by the meter.footprint users can choose from many content distribution options - some simple, some advanced - for different parts of a site, while freeflow optimizes everything automatically. footprint also distributes html, not just embedded files, spreading database information through the network. akamai leaves html at home.under unusual circumstances, footprint may be less bulletproof than freeflow, but it has proven itself well. it kept the starr report available on the los angeles times site, when many others buckled, and served intuit's site reliably all through tax season. as long as speed is scarce on the net, the two companies are going to find fans.both companies, if they continue to grow, will route traffic more efficiently over the net as a whole, increasing delivery speeds for subscribers and nonsubscribers as well. but there's a downside. content delivered via these subscription-based networks will make content routed by the old, free internet seem slow. a page that loads in six seconds seems fine until you visit one that loads in four. this effect will be magnified as people upgrade their connections to dsl and broadband cable. with fatter pipes, users will demand more information-intensive experiences, and the newly available last-mile bandwidth will be filled up with fast, dazzling content from supersites, served from networks like akamai. espn.com will appear instantly, but you'll have to wait an age for anything homegrown or poorly financed.others argue, however, that the internet is already a tiered environment, where cash-rich content providers can add more and more hardware to improve delivery, while your cousin's homepage, with no traffic management resources behind it, is already slow. akamai and sandpiper just allow more publishers to tap into premium services.either way you look at it, the stakes are high. the winner, if there is one, will have its hand in the major revenue-generating sites on the web. more than any other company in the medium's short history, the winner will own the net - or at least the parts of it that pay.sea changeakamaiians compare their company to fedex, delivering content faster and more reliably than the old usps. it's not a bad comparison - as good, at least, as the glorious advent of sea travel thousands of years before christ - but it raises the specter of huge capital needs and about 10 more years till ubiquity.akamai wants to be everywhere: says paul, \"a server in every pop.\"yet in less than a year, the little-known company is well on its way toward global domination. akamai wants to be universal, as widespread as the net itself, with, in paul's terms, \"a server in every pop.\" if the company has its way, every computer on the net will connect to akamai servers, which will push and pull content around with the tides, constantly running calculations on the turbulence and fluid dynamics of information. as each new site and isp signs on, akamai's ocean will swell, carrying ships ever closer to their final goals. for now at least, the ordinary user ought to be glad that the company in charge of the earth's oceans wants only to give each of us beachfront property.plusakamai technologies"}