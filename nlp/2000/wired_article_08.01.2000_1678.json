{"URL": "https://www.wired.com/2000/08/comcomp", "heading": "you got the power", "subheading": "next comes the payoff. a wave of startups is poised to harvest the network's most wasted resource: your idle cpu cycles. nelson minar isn't afraid to think big. the cto of san francisco-based popular power has a dream of linking millions of otherwise idle pcs worldwide to perform monumental computations, building a supercomputer out of [\u2026]", "author": "wired staff", "category": "not found", "type": "article", "timestamp": "08.01.2000 12:00 PM", "text": "__next comes the payoff. a wave of startups is poised to harvest the network's most wasted resource: your idle cpu cycles. __ nelson minar isn't afraid to think big. the cto of san francisco-based popular power has a dream of linking millions of otherwise idle pcs worldwide to perform monumental computations, building a supercomputer out of unemployed resources.minar's global vision has humble beginnings. popular power's spartan headquarters feature a few thrift-store rugs and secondhand desks, a wall map of the internet, and a couple of computers. there's no conference table and not even enough chairs, so minar stands and delivers his spiel.\"the internet is lifeless if the only thing we do with it is display web pages,\" says minar, a former mit media lab researcher. \"that's what we're after - bringing the internet to life.\" when minar says \"we,\" he's actually referring to millions of folks whom he hopes will voluntarily lend him the use of their computers.in january, minar and his onetime reed college classmate marc hedlund founded popular power (www.popularpower.com), the first commercial distributed computation company to release software allowing any participating business to do work on its platform. to minar & co., community computation isn't just a budding business. it's a cause.__bioengineers, mathematicians, and cryptographers already take community computation seriously. big business may be next. __at its most basic level, distributed processing is a way of harvesting a resource that until now has been squandered on a massive scale: unused cpu cycles. even if you type two characters per second on your keyboard, you're using only a fraction of your machine's power. during that second, most desktop computers can simultaneously perform hundreds of millions of additional operations. time-sharing computers of the 1960s exploited this ability, allowing hundreds of people to use the same mainframe. more recently, millions of pcs around the world have been banded together on the net to create, with their downtime, ad hoc supercomputers.these multi-teraflop cpu communities, harnessing trillions of floating-point operations per second, have joined forces most famously for the seti@home project, which unites the computing power of millions of pcs around the globe to search for signs of extraterrestrial intelligence.but community computation is also at work in less celebrated arenas, providing the muscle to analyze protein-folding problems in bioengineering, large prime numbers in cryptography, and planetary climate models. others see distributed processing aiding in the design of synthetic drugs, an effort that requires trial simulations on billions of possible molecules.and big business may be next. several startups are experimenting with ways to link idle computers cheaply, selling off the resources to the highest bidder. community computation came into existence because enough people thought it would be a cool thing to do. distributed processing's second wave could lead not only to new industries, but also to new ways of thinking about computation.you know you've made it to the front door of seti@home when you come across the welcome all species doormat. it's here, at the university of california space sciences laboratory in the berkeley hills, where you'll find the mothership of community computation, the largest cooperative computing effort in the world.more than 2 million computers worldwide participate in seti@home, donating untapped cpu time to analyze signals collected by a radio telescope in puerto rico. the telescope pulls down about 50 gbytes of data per day, far more than seti's servers can analyze. that's where community computing comes in. seti@home participants install client software that analyzes a tiny portion of the signal, while functioning like a screensaver. the software downloads a small segment of radio telescope signals and processes it, looking for interesting patterns consistent with intelligent life. when the task is complete, the program uploads the results to seti@home headquarters and collects a new chunk of digitized space signal to search.seti@home's global network of pcs can perform 12 teraflops, easily beating today's fastest supercomputers, which top out at about 3 teraflops. david p. anderson, seti@home's director, shows me the rack that houses the project's three servers, nicknamed sagan, asimov, and cyclops.\"sagan sends out the work units,\" anderson explains, pointing to one of the sun enterprise 450 servers. \"cyclops and asimov both run database servers. when the client program gets one of these batches of radio telescope data and spends however many hours analyzing digital patterns, it sends back a shortlist of interesting things it found. those are pattern matches, a shape of the noise that is similar to what we're looking for. each of these interesting finds is entered into the database. we have already accumulated about a half-billion of these candidate signals.\"the idea for seti@home arose in 1995, when anderson was contacted by a former berkeley grad student named david gedye, who'd become the cto of apex learning. inspired by documentaries about the apollo space program, which made people all over the world feel that human beings were taking a collective step forward, gedye wondered whether a project today could have a similar impact. he hit upon the idea of harnessing the public's fascination with both the internet and the seti program to form seti@home.it took nearly three years to raise enough money to develop the software, and in mid-1999, unix, windows, and macintosh versions of the seti@home client were released within a month of one another. \"it's been a wild ride since then,\" says anderson. \"we were hoping for at least 100,000 people worldwide to get enough computer power to make the thing worthwhile. after a week, we had 200,000 participants, after four or five months it broke through a million, and now it's past 2 million.\"seti@home is community computation's first true success story, having already racked up the equivalent of 280,000 years of processing time. community computation enthusiasts hope it may lead to more-ambitious projects that let computers share not only cpu cycles but also memory, disk space, and network bandwidth.anderson sees seti not merely as a global computational exercise, but as a gathering place for new communities.\"several people are running seti@home in the same town in poland,\" he says. \"we'd love to help them find out about each other. we haven't exhausted the potential for connecting people as well as computers.\"the roots of community computation stretch back further than the creation of seti@home. i first encountered the idea in the early 1980s, while conducting research in the library of xerox parc. some of the most interesting reading was in the distinctive blue-and-white bound documents of internal parc research reports. one, i remember, had an intriguing title: \"notes on the 'worm' programs - some early experience with a distributed computation.\" the report, by john f. shoch and jon a. hupp, detailed experiments with a computer program that traveled from machine to machine on a local network, looking for idle cpus, sneaking in computations on any unused machine, then retreating with the results when a human started tapping on the keyboard - a local-area version of what seti@home would accomplish on a global scale two decades later.__\"computation is becoming a commodity. if pixar needs to do toy story 3, instead of buying new machines, they can bid on the cycle market.\" __a 1975 sci-fi novel, the shockwave rider, by john brunner, was part of the inspiration for the parc experiment. in the book, brunner describes an omnipotent \"tapeworm\" program running loose through a network. brunner's worm is malignant, but the parc scientists imagined a beneficial program that would move from machine to machine harnessing resources.the parc worm experiments took place on about 100 alto computers linked via the first ethernet. shoch and hupp's worms were able to roam through the network, reproducing in the memories of idle machines. each segment performed a computation and had the power to reproduce and transmit clones to other nodes of the network. shoch and hupp created \"billboard worms\" that distributed graphic images - a popular means of getting \"the cartoon of the day.\" another prescient experiment was in \"multi-machine animation\": shared computations for rendering realistic computer graphics.doing this sort of distributed computation on a global scale wasn't possible in the early 1980s. the net's population was measured in the thousands, and the bandwidth was puny. but by 1989, richard crandall, now distinguished scientist at apple (and once my roommate at reed college), started networking next computers to find, factor, and test gargantuan prime numbers.\"community supercomputing occurred to me one day at next engineering headquarters,\" crandall recalls. \"i thought we ought to make these machines do what they were designed to do, which is to work when we humans are not working. machines have no business sleeping.\"crandall installed software that allowed idle next machines to perform computations, combining their efforts across the network. he called this software godzilla, but after a legal inquiry from the company that owned the rights to the movie character, he renamed it zilla. crandall put zilla to work on huge prime numbers, which are crucial in cryptography. it was then used to test a new encryption scheme at next - a scheme now employed at apple, which acquired next. in 1991, zilla won the computerworld smithsonian award for science.later, crandall and several colleagues used distributed processing to complete the deepest computation ever performed, asking the question: is the 24th fermat number (which has more than 5 million digits) prime? \"it took 10**17 machine operations - 100 quadrillion,\" crandall says proudly. \"with that level of computational effort, you can create a full-length movie. in fact, that's about the same number of operations pixar required to render a bug's life.\"the day when animated movies are rendered with the help of networked pcs may be closer than many would've dreamed possible even a few years ago.\"computation is becoming a commodity,\" says david mcnett of distributed.net, a community computation effort that focuses on encryption cracking, as well as on scientific and academic work. \"in the future, if pixar needs to do toy story 3, instead of buying new machines, they can bid on the cycle market. that's what we're working toward.\"distributed.net is a loose coalition of mathematicians, programmers, and cypherpunks who joined forces in 1997 to decrypt a puzzle devised by rsa security, a leading vendor of encryption software that regularly issues open challenges to break its code. the distributed.net team wound up collecting the $10,000 grand prize and plowed the money into a nonprofit research foundation.\"we knew that this technology could have practical uses,\" says mcnett, \"but there was little know-how to stimulate applications, so we did research by trial and error.\"at its heart, distributed.net is an online community. the 15 founders met via irc, and most have never seen one another in the flesh. members are scattered across north america, europe, south america, and asia, and board meetings take place on an irc channel. says mcnett, \"we network people as well as computers.\"since its first successes with cryptographic challenges, distributed.net has moved on to work with the uk's sanger centre on human genome mapping, and boasts a community of 60,000 participants with 200,000 computers. distributed.net's computing power now equals that of more than 180,000 pentium ii 266-mhz computers working 24/7, and the infrastructure can reliably handle tens of thousands of additional new computers.although the group is concentrating on not-for-profit research, mcnett believes distributed computation is well suited to commercial applications. in the future, it could easily involve any organization with a large number of pcs: \"think of all the computers that exxon or coca-cola own that sit idle all night and on weekends,\" he says.adam l. beberg, a founder of distributed.net, started playing with cooperative computation over local networks in 1991, and designed his first network-distributed computation system in 1995. now he's distributing developer toolkits so that anyone can build a community computation network. he calls his work-in-progress cosm (cosm.mithral.com), a distributed processing architecture that will dispense with centralized servers. in cosm's universe, all clients are servers for other clients on the network - similar to the way napster and gnutella work.distributed supercomputers may turn out to be useful laboratories for studying long-term environmental problems. myles r. allen of the rutherford appleton laboratory in chilton, england, has proposed that distributed computation be applied to the notoriously difficult task of climate simulation (www.climate-dynamics.rl.ac.uk). last fall, allen published a request appealing to the web's civic spirit: \"this experiment would introduce an entirely new form of climate prediction: a fuzzy prediction, reflecting the range of risks and probabilities, rather than a single 'best guess' forecast. and we don't have the computing resources to do this any other way. so, if you're lucky enough to have a powerful pc on your desk or at home, we're asking you to do your bit so the right decisions get taken on climate change.\" allen received 15,000 replies within two weeks.opencola is another community computing effort, an open source distributed search tool that uses idle cpu cycles to help maintain a noncommercial index of the web's contents. unlike commercial search engines, which pay for the necessary tasks of spidering the web to update their indexes, opencola (www.opencola.com) relies on each participant in the community to pick part of the web to spider in their spare time. beyond searching and spidering, opencola could let open source enthusiasts join on-the-fly distributed networks. when one computer on the network learns that it has been given a computing task that lends itself to distribution (such as database conversion, image manipulation and rendering, or file-format conversion), it can poll the other computers connected to the network to find clients available to help.the next hurdle for community computation is finding the right business model to turn this now largely social enterprise into a viable industry. popular power, for example, is positioning itself as an exchange for buyers and sellers of computing time.mark hedlund, popular power's ceo, got the idea for the company after attending a presentation about seti@home. hedlund, who set up the internet division for lucasfilm, contacted his college buddy nelson minar, who'd been working on experiments at the media lab using independent computing resources and agents.__multi-teraflop cpu communities already drub supercomputers - and think of all the pcs at exxon that sit idle all night and on weekends. __\"the idea of trying to profit from distributed computation kept coming back into the conversation,\" hedlund says. the pair contacted another friend formerly at industrial light & magic, wondering whether selling cpu hours to render special effects was a viable business proposition. when his friend confirmed the need, hedlund recalls, \"i called a biotech firm, a chemical company, an environmental agency, a couple more entertainment companies, and they were all interested. i'm convinced there's a market.\"some of popular power's early investors have been software engineers yearning to do something meaningful. \"a lot of engineers i talk to are tired of building ecommerce systems,\" hedlund says. \"they recognize that cooperative computing is what the internet is meant to do. we've had no problem recruiting top talent.\"as for the service, it works much the way seti@home does. a popular power participant downloads software that functions as a screensaver, activating when the user isn't operating the computer. it goes to work on a piece of a large computing task and sends the results to popular power. when the user moves the mouse or presses a key, the software immediately suspends its community work. as part of a preview release, popular power is donating computing power to nonprofit ventures, as well as several commercial ones. for the company's first project, hedlund hired a programmer who plans to use community computation to test influenza vaccine designs against epidemic data.rather than paying participants directly in cash, popular power hopes to partner with users' isps to knock $10 or so off their monthly fee or provide a gift certificate for an online store. participants will receive a ranking based on the work they do for the system. when two users' computers are available for work, the user with the higher ranking will get first crack at the better-paying jobs. participants can maximize earnings by working on commercial projects, or donate time to nonprofit projects, or mix the two. to allay security concerns, the software encloses programs in a container called a \"sandbox\" that limits what they are allowed to do and blocks access to the files on users' computers.popular power is targeting businesses that do intensive computing - insurance companies, pharmaceutical giants - and is hoping to sell them on reducing their costs. ordinarily, such a company would buy an expensive supercomputer or cluster, and then pay for maintenance and administration. the supercomputer depreciates almost as soon as it's put into service, and quickly competes against newer models. community computation, however, would allow companies to buy only the computing power they need for individual projects, greatly reducing the cost. a small business could compete with a much larger one on computing projects without having to invest millions in infrastructure. and while today's fastest supercomputers top out at around 3 teraflops, popular power expects to be able to exceed that limit many times over.a rival business model for community computation bears a resemblance to multilevel marketing. processtree network, a huntsville, alabama-based distributed processing startup, plans to sell cycles and reward participants for their cpu hours, then continue to reward them for signing up additional partners, and for the partners their recruits sign up. with plenty of computing power on hand to keep track of who is owed a microcommission, a multilevel cpu marketing scheme could well be viable.jim albea, founder of processtree, is a participant in seti@home and another voluntary effort, the great internet mersenne prime search. originally trained as an architect, albea now works in software development for intergraph, a huntsville company that built the architectural software he had been using. in the late 1980s, albea had worked on a lan-based distributed processing product for intergraph that built animated video.\"i had been involved in voluntary projects, but had not thought much about the commercial applications,\" albea recalls. \"when the mersenne prime was found in 1999, it hit me that distributed computation is poised to go commercial. i'm always looking for the next big idea. it felt like i had been asleep at the switch.\"he launched the processtree web site (www.processtree.com) in january; the company merged with another service, dcypher.net, in april. processtree aggregates and cultivates the user network, while dcypher.net will develop the technology.as of june, about 29,000 people and more than 56,000 computers had signed up. according to albea, \"we're adding around 300 new people and 500 new computers every day.\"albea says that the real power of the enterprise lies in the community of volunteers, and the founders intend to keep the core team small. \"we don't have much overhead,\" says albea. \"the big companies get themselves all wound around their axles trying to figure out the market, but we're going to create markets. the application that excites me most is the one i haven't thought of yet.\" so far, albea doesn't have any clients he's ready to name, but says several fortune 500 companies and animation houses are interested.community computation has its skeptics - bob metcalfe, the inventor of ethernet and founder of 3com, for one. a parc veteran himself, metcalfe has followed the distributed processing concept for decades, and calls it \"one of my favorite ideas.\" but while metcalfe sees progress on many fronts, he isn't convinced community computation can become a commercial venture.\"there aren't that many computational problems amenable to this kind of loosely connected parallelism,\" metcalfe points out in an email. \"the costs of transporting the data, program, and results of most computations overwhelm the benefits of the parallelism, and people with serious computations are not likely to trust results coming from unreliable machines owned by total strangers. besides, the costs of computation keep going down, so why bother trying to recycle the waste of this renewable resource?\"but apple scientist richard crandall believes there are plenty of computational problems to make distributed processing viable. \"the problems are out there, but the software is insufficient right now to optimize community computation,\" he says. \"in principle, the costs are very low. someone simply has to work out a proper business model and everything will follow.\"popular power's hedlund also disagrees that the costs of transporting data over a distributed network overwhelm the benefits. \"network speed and connectivity have finally reached the point where it's completely feasible to do nontrivial work,\" he says. \"i think that seti@home demonstrates this nicely.\"__\"big companies try to figure out the market, but we're going to create markets. the most exciting application is the one i haven't thought of yet.\" __sharable resources aren't limited to cpus. uncounted terabytes of disk space and peripheral devices are often as idle as most cpus. within a few years, the world's computers will number in the billions - and tomorrow's models will be orders of magnitude more powerful than today's. but no matter how brawny individual computers become, they will never be as powerful individually as collectively. community computation could be another amateur enterprise that grows into an industry, a disruptive phenomenon that changes everything from scientific research to entertainment. reclaiming the world's idle computing resources could fuel the next stage of computing, the way moore's law has for the past 30 years.and it could all happen while you've stepped away from your computer."}